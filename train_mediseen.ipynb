{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b4162c",
   "metadata": {},
   "source": [
    "## Configure Single GPU Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from utils import AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "from evaluation import validation\n",
    "\n",
    "# --- For reproducibility ---\n",
    "random.seed(1111)\n",
    "np.random.seed(1111)\n",
    "torch.manual_seed(1111)\n",
    "torch.cuda.manual_seed(1111)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d01c44",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        # --- Paths & Naming ---\n",
    "        self.exp_name = \"TPS-ResNet-BiLSTM-Attn-FineTune-Mediseen\"\n",
    "        self.train_data = \"data/train\"\n",
    "        self.valid_data = \"data/val\"\n",
    "        self.saved_model = \"saved_models/TPS-ResNet-BiLSTM-Attn.pth\"\n",
    "\n",
    "        # --- Training Specs ---\n",
    "        self.manualSeed = 1111\n",
    "        self.workers = 4\n",
    "        self.batch_size = 64\n",
    "        self.num_iter = 25000\n",
    "        self.valInterval = 500\n",
    "        self.FT = True\n",
    "        self.adam = True\n",
    "        self.lr = 0.0001\n",
    "        self.beta1 = 0.9\n",
    "        self.rho = 0.95\n",
    "        self.eps = 1e-8\n",
    "        self.grad_clip = 5\n",
    "\n",
    "        # --- Data Processing ---\n",
    "        self.select_data = \"train\"\n",
    "        self.batch_ratio = \"1.0\"\n",
    "        self.total_data_usage_ratio = \"1.0\"\n",
    "        self.batch_max_length = 25\n",
    "        self.imgH = 32\n",
    "        self.imgW = 100\n",
    "        self.rgb = False  # GRAY ONLY\n",
    "        self.character = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "        self.sensitive = False\n",
    "        self.PAD = True\n",
    "        self.data_filtering_off = False\n",
    "\n",
    "        # --- Model Architecture ---\n",
    "        self.Transformation = \"TPS\"\n",
    "        self.FeatureExtraction = \"ResNet\"\n",
    "        self.SequenceModeling = \"BiLSTM\"\n",
    "        self.Prediction = \"Attn\"\n",
    "        self.num_fiducial = 20\n",
    "        self.input_channel = 3 if self.rgb else 1\n",
    "        self.output_channel = 512\n",
    "        self.hidden_size = 256\n",
    "\n",
    "        # --- GPU ---\n",
    "        self.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "opt = ModelConfig()\n",
    "\n",
    "# Create directory for saving the fine-tuned models\n",
    "os.makedirs(f\"./saved_models/{opt.exp_name}\", exist_ok=True)\n",
    "\n",
    "# Update character set for case-sensitive models\n",
    "if opt.sensitive:\n",
    "    opt.character = string.printable[:-6]\n",
    "\n",
    "# Define the converter to calculate num_class\n",
    "converter = AttnLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)  # This creates the necessary attribute\n",
    "\n",
    "# Handle multi-GPU settings\n",
    "if opt.num_gpu > 1:\n",
    "    print(f\"Using {opt.num_gpu} GPUs\")\n",
    "    opt.workers = opt.workers * opt.num_gpu\n",
    "    opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "\n",
    "# ✨ FIX: Initialize the model here AFTER all opt attributes are set\n",
    "model = Model(opt)\n",
    "print(\"Model configured successfully.\")\n",
    "print(\n",
    "    \"Model input parameters:\",\n",
    "    opt.imgH,\n",
    "    opt.imgW,\n",
    "    opt.num_fiducial,\n",
    "    opt.input_channel,\n",
    "    opt.output_channel,\n",
    "    opt.hidden_size,\n",
    "    opt.num_class,\n",
    "    opt.batch_max_length,\n",
    "    opt.Transformation,\n",
    "    opt.FeatureExtraction,\n",
    "    opt.SequenceModeling,\n",
    "    opt.Prediction,\n",
    ")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"FeatureExtraction\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        # Ensure all other layers are trainable\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57d0c9",
   "metadata": {},
   "source": [
    "## Load Data to GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ace145",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading pretrained model from {opt.saved_model}\")\n",
    "try:\n",
    "    # Load the state dictionary from the file\n",
    "    state_dict = torch.load(opt.saved_model, map_location=device)\n",
    "\n",
    "    # Create a new dictionary without the 'module.' prefix\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the cleaned state dictionary\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print(\"✅ Successfully loaded and cleaned pre-trained model weights.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Training from scratch instead.\")\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "# DataParallel for multi-GPU\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# --- Loss, Averager, and Optimizer ---\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(\n",
    "    device\n",
    ")  # ignore [GO] token = 0\n",
    "loss_avg = Averager()\n",
    "\n",
    "# Filter that only require gradient descent\n",
    "filtered_parameters = []\n",
    "params_num = []\n",
    "for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "    filtered_parameters.append(p)\n",
    "    params_num.append(np.prod(p.size()))\n",
    "print(\"Trainable params num : \", sum(params_num))\n",
    "\n",
    "# Setup optimizer\n",
    "if opt.adam:\n",
    "    # ✨ ADD weight_decay HERE ✨\n",
    "    optimizer = optim.Adam(\n",
    "        filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=0.001\n",
    "    )\n",
    "else:\n",
    "    optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=5, factor=0.5)\n",
    "\n",
    "print(\"Optimizer:\")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initializing Dataloaders ---\")\n",
    "\n",
    "# --- Dataloaders ---\n",
    "# Validation loader\n",
    "AlignCollate_valid = AlignCollate(\n",
    "    imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD\n",
    ")\n",
    "valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.workers),\n",
    "    collate_fn=AlignCollate_valid,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(valid_dataset_log)\n",
    "\n",
    "# Split the configuration strings into lists\n",
    "opt.select_data = opt.select_data.split(\"-\")\n",
    "opt.batch_ratio = opt.batch_ratio.split(\"-\")\n",
    "\n",
    "# Training loader\n",
    "train_dataset = Batch_Balanced_Dataset(opt)\n",
    "print(\"Dataloaders initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f9f44c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     46\u001b[39m model.eval()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     48\u001b[39m     (\n\u001b[32m     49\u001b[39m         valid_loss,\n\u001b[32m     50\u001b[39m         current_accuracy,\n\u001b[32m     51\u001b[39m         current_norm_ED,\n\u001b[32m     52\u001b[39m         preds,\n\u001b[32m     53\u001b[39m         confidence_score,\n\u001b[32m     54\u001b[39m         labels,\n\u001b[32m     55\u001b[39m         infer_time,\n\u001b[32m     56\u001b[39m         length_of_data,\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     ) = \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Store metrics\u001b[39;00m\n\u001b[32m     60\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m\"\u001b[39m].append(iteration + \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/artus/mediseen-oss/evaluation.py:121\u001b[39m, in \u001b[36mvalidation\u001b[39m\u001b[34m(model, criterion, evaluation_loader, converter, opt)\u001b[39m\n\u001b[32m    118\u001b[39m     preds_str = converter.decode(preds_index.data, preds_size.data)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_for_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     forward_time = time.time() - start_time\n\u001b[32m    124\u001b[39m     preds = preds[:, :text_for_loss.shape[\u001b[32m1\u001b[39m] - \u001b[32m1\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:192\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     module_kwargs = ({},)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.device_ids) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m    194\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.parallel_apply(replicas, inputs, module_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/artus/mediseen-oss/model.py:66\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, input, text, is_train)\u001b[39m\n\u001b[32m     63\u001b[39m     contextual_feature = visual_feature  \u001b[38;5;66;03m# for convenience. this is NOT contextually modeled by BiLSTM\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Prediction stage \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m prediction = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontextual_feature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mediseen-oss/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/artus/mediseen-oss/modules/prediction.py:33\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, batch_H, text, is_train, batch_max_length)\u001b[39m\n\u001b[32m     30\u001b[39m batch_size = batch_H.size(\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m num_steps = batch_max_length + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# +1 for [s] at end of sentence.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m output_hiddens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m hidden = (torch.FloatTensor(batch_size, \u001b[38;5;28mself\u001b[39m.hidden_size).fill_(\u001b[32m0\u001b[39m).to(device),\n\u001b[32m     35\u001b[39m           torch.FloatTensor(batch_size, \u001b[38;5;28mself\u001b[39m.hidden_size).fill_(\u001b[32m0\u001b[39m).to(device))\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_accuracy = -1\n",
    "best_norm_ED = -1\n",
    "\n",
    "# To store metrics for plotting\n",
    "history = {\n",
    "    \"iterations\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"norm_ED\": [],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "for iteration in range(opt.num_iter):\n",
    "    model.train()\n",
    "    # --- Train one iteration ---\n",
    "    image_tensors, labels = train_dataset.get_batch()\n",
    "    image = image_tensors.to(device)\n",
    "    text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "    batch_size = image.size(0)\n",
    "\n",
    "    preds = model(image, text[:, :-1])  # Exclude the [s] token for input\n",
    "    target = text[:, 1:]  # Exclude the [GO] token for target\n",
    "\n",
    "    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "    # l1_lambda = 1e-6  # Regularization strength\n",
    "    # l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "    # cost = cost + l1_lambda * l1_penalty\n",
    "\n",
    "    model.zero_grad()\n",
    "    cost.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg.add(cost)\n",
    "\n",
    "    # --- Validation part ---\n",
    "    if (iteration + 1) % opt.valInterval == 0 or iteration == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n--- Validation at Iteration {iteration + 1}/{opt.num_iter} ---\")\n",
    "\n",
    "        # Switch to evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                valid_loss,\n",
    "                current_accuracy,\n",
    "                current_norm_ED,\n",
    "                preds,\n",
    "                confidence_score,\n",
    "                labels,\n",
    "                infer_time,\n",
    "                length_of_data,\n",
    "            ) = validation(model, criterion, valid_loader, converter, opt)\n",
    "\n",
    "        # Store metrics\n",
    "        history[\"iterations\"].append(iteration + 1)\n",
    "        history[\"train_loss\"].append(loss_avg.val())\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"accuracy\"].append(current_accuracy)\n",
    "        history[\"norm_ED\"].append(current_norm_ED)\n",
    "\n",
    "        # Logging\n",
    "        loss_log = f\"Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}s\"\n",
    "        loss_avg.reset()\n",
    "\n",
    "        current_model_log = (\n",
    "            f\"Accuracy: {current_accuracy:0.3f} | Norm_ED: {current_norm_ED:0.2f}\"\n",
    "        )\n",
    "        print(loss_log)\n",
    "        print(current_model_log)\n",
    "\n",
    "        # Save the best models\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            torch.save(\n",
    "                model.state_dict(), f\"./saved_models/{opt.exp_name}/best_accuracy.pth\"\n",
    "            )\n",
    "            print(f\"✨ New best accuracy! Saved to best_accuracy.pth\")\n",
    "        if current_norm_ED > best_norm_ED:\n",
    "            best_norm_ED = current_norm_ED\n",
    "            torch.save(\n",
    "                model.state_dict(), f\"./saved_models/{opt.exp_name}/best_norm_ED.pth\"\n",
    "            )\n",
    "            print(f\"✨ New best Norm ED! Saved to best_norm_ED.pth\")\n",
    "\n",
    "        best_model_log = (\n",
    "            f\"Best Accuracy: {best_accuracy:0.3f} | Best Norm_ED: {best_norm_ED:0.2f}\"\n",
    "        )\n",
    "        print(best_model_log)\n",
    "\n",
    "        # Show some predicted results\n",
    "        dashed_line = \"-\" * 80\n",
    "        head = f\"{'Ground Truth':25s} | {'Prediction':25s} | Confidence Score & T/F\"\n",
    "        predicted_result_log = f\"{dashed_line}\\n{head}\\n{dashed_line}\\n\"\n",
    "        for gt, pred, confidence in zip(labels[:5], preds[:5], confidence_score[:5]):\n",
    "            gt = gt[: gt.find(\"[s]\")]\n",
    "            pred = pred[: pred.find(\"[s]\")]\n",
    "            predicted_result_log += (\n",
    "                f\"{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n\"\n",
    "            )\n",
    "        predicted_result_log += f\"{dashed_line}\"\n",
    "        print(predicted_result_log)\n",
    "\n",
    "\n",
    "print(\"\\n--- End of Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "fig.suptitle(\"Model Training and Validation Metrics\", fontsize=16)\n",
    "\n",
    "# --- Plot 1: Loss ---\n",
    "axes[0].plot(\n",
    "    history_df[\"iterations\"],\n",
    "    history_df[\"train_loss\"],\n",
    "    label=\"Training Loss\",\n",
    "    color=\"blue\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    history_df[\"iterations\"],\n",
    "    history_df[\"valid_loss\"],\n",
    "    label=\"Validation Loss\",\n",
    "    color=\"orange\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "axes[0].set_title(\"Training vs. Validation Loss\")\n",
    "axes[0].set_xlabel(\"Iterations\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 2: Accuracy ---\n",
    "axes[1].plot(\n",
    "    history_df[\"iterations\"],\n",
    "    history_df[\"accuracy\"],\n",
    "    label=\"Validation Accuracy\",\n",
    "    color=\"green\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "axes[1].set_title(\"Validation Accuracy\")\n",
    "axes[1].set_xlabel(\"Iterations\")\n",
    "axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# --- Plot 3: Normalized Edit Distance ---\n",
    "axes[2].plot(\n",
    "    history_df[\"iterations\"],\n",
    "    history_df[\"norm_ED\"],\n",
    "    label=\"Validation Norm ED\",\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "axes[2].set_title(\"Validation Normalized Edit Distance\")\n",
    "axes[2].set_xlabel(\"Iterations\")\n",
    "axes[2].set_ylabel(\"Norm ED\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906aecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Final Evaluation on the Best Model ---\")\n",
    "\n",
    "# Create a fresh model instance and load the best saved weights\n",
    "final_model = Model(opt)\n",
    "final_model = torch.nn.DataParallel(final_model).to(device)\n",
    "\n",
    "best_model_path = f\"./saved_models/{opt.exp_name}/best_accuracy.pth\"\n",
    "print(f\"Loading weights from: {best_model_path}\")\n",
    "final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# Run validation\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_loss, accuracy, norm_ED, _, _, _, _, _ = validation(\n",
    "        final_model, criterion, valid_loader, converter, opt\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Final Performance on Validation Set ---\")\n",
    "print(f\"  -> Final Accuracy: {accuracy:.3f}%\")\n",
    "print(f\"  -> Final Normalized Edit Distance: {norm_ED:.3f}\")\n",
    "print(f\"  -> Final Validation Loss: {valid_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7833ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from dataset import ResizeNormalize, NormalizePAD\n",
    "\n",
    "\n",
    "def run_inference(model, image_path, opt, converter):\n",
    "    \"\"\"\n",
    "    Loads an image, preprocesses it, and runs inference to predict the text.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image not found at {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # --- Image Preprocessing ---\n",
    "    image = Image.open(image_path).convert(\"RGB\" if opt.rgb else \"L\")\n",
    "\n",
    "    if opt.PAD:\n",
    "        # Replicate the logic from AlignCollate for padded resize\n",
    "        resized_max_w = opt.imgW\n",
    "        input_channel = 3 if opt.rgb else 1\n",
    "        transform = NormalizePAD((input_channel, opt.imgH, resized_max_w))\n",
    "\n",
    "        w, h = image.size\n",
    "        ratio = w / float(h)\n",
    "        if math.ceil(opt.imgH * ratio) > opt.imgW:\n",
    "            resized_w = opt.imgW\n",
    "        else:\n",
    "            resized_w = math.ceil(opt.imgH * ratio)\n",
    "\n",
    "        resized_image = image.resize((resized_w, opt.imgH), Image.BICUBIC)\n",
    "        image_tensor = transform(resized_image)\n",
    "    else:\n",
    "        transform = ResizeNormalize((opt.imgW, opt.imgH))\n",
    "        image_tensor = transform(image)\n",
    "\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # --- Model Inference ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_size = image_tensor.size(0)\n",
    "        # Dummy inputs for attention decoder\n",
    "        text_for_pred = (\n",
    "            torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "        )\n",
    "\n",
    "        preds = model(image_tensor, text_for_pred, is_train=False)\n",
    "\n",
    "        # Select max probability (greedy decoding)\n",
    "        _, preds_index = preds.max(2)\n",
    "\n",
    "        # Decode index to character string\n",
    "        preds_str = converter.decode(\n",
    "            preds_index, torch.IntTensor([opt.batch_max_length] * batch_size)\n",
    "        )\n",
    "\n",
    "    # --- Post-processing ---\n",
    "    pred_text = preds_str[0]\n",
    "    pred_EOS = pred_text.find(\"[s]\")\n",
    "    if pred_EOS != -1:\n",
    "        pred_text = pred_text[:pred_EOS]\n",
    "\n",
    "    return pred_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c408bf",
   "metadata": {},
   "source": [
    "```\n",
    "# --- Example Usage ---\n",
    "# 1. Create a folder named `test_images` in your project root.\n",
    "# 2. Add some sample images to it.\n",
    "# 3. Update the `sample_image_path` variable below.\n",
    "\n",
    "# Make a dummy test image if it doesn't exist\n",
    "os.makedirs('test_images', exist_ok=True)\n",
    "sample_image_path = 'test_images/sample1.png'\n",
    "if not os.path.exists(sample_image_path):\n",
    "    try:\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        img = Image.new('RGB', (200, 60), color = (255, 255, 255))\n",
    "        d = ImageDraw.Draw(img)\n",
    "        d.text((10,10), \"hello world\", fill=(0,0,0))\n",
    "        img.save(sample_image_path)\n",
    "        print(f\"Created a dummy test image at: {sample_image_path}\")\n",
    "    except:\n",
    "        print(f\"Please place a test image at {sample_image_path}\")\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    predicted_text = run_inference(final_model, sample_image_path, opt, converter)\n",
    "    \n",
    "    # Display the image and the prediction\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    img = Image.open(sample_image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Prediction: \"{predicted_text}\"', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediseen-oss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
